{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from data.data import LoadData\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "num_atom_type = 28 # KNOWN META-INFO ABOUT ZINC DATASET\n",
    "pos_emb_dim = 8 # the minimal number of nodes in the dataset is 9\n",
    "batch_size = 256\n",
    "nhead = 4\n",
    "\n",
    "pad_val = num_atom_type\n",
    "cls_val = num_atom_type+1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "reduced_dataset = True\n",
    "\n",
    "verbose = False\n",
    "def vprint(txt):\n",
    "    if verbose:\n",
    "        print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Loading dataset ZINC...\n",
      "train, test, val sizes : 10000 1000 1000\n",
      "[I] Finished loading.\n",
      "[I] Data load time: 10.5564s\n"
     ]
    }
   ],
   "source": [
    "dataset = LoadData('ZINC')\n",
    "dataset_train = dataset.train\n",
    "dataset_val = dataset.val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [ True, False,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False,  True, False,  True, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False,  True, False,  True, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False,  True, False,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False,  True, False,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False,  True, False,  True, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False,  True, False,  True, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False,  True, False,  True,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False,  True, False,\n",
       "         False,  True, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "          True, False,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True, False,  True, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True, False,  True, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False,  True, False,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False,  True, False,  True, False, False, False,\n",
       "         False, False, False, False, False,  True, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True, False,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True, False, False, False,\n",
       "          True, False, False, False,  True, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "         False,  True, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "          True, False,  True,  True, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False,  True, False, False,  True, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False,  True,\n",
       "         False, False, False,  True, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False,  True, False, False, False],\n",
       "        [False, False, False, False, False,  True, False, False, False, False,\n",
       "          True, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False,  True, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False,  True, False]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = dataset_train[0][0].adjacency_matrix()\n",
    "tmp = adj.to_dense() != 0\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def laplacian_positional_encoding(g, pos_enc_dim):\n",
    "    \"\"\"\n",
    "        Graph positional encoding v/ Laplacian eigenvectors\n",
    "    \"\"\"\n",
    "\n",
    "    # Laplacian\n",
    "    A = g.adjacency_matrix_scipy(return_edge_ids=False).astype(float)\n",
    "    N = sp.diags(dgl.backend.asnumpy(g.in_degrees()).clip(1) ** -0.5, dtype=float)\n",
    "    L = sp.eye(g.number_of_nodes()) - N * A * N\n",
    "\n",
    "    # Eigenvectors with numpy\n",
    "    EigVal, EigVec = np.linalg.eig(L.toarray())\n",
    "    idx = EigVal.argsort() # increasing order\n",
    "    EigVal, EigVec = EigVal[idx], np.real(EigVec[:,idx])\n",
    "    g.ndata['lap_pos_enc'] = torch.from_numpy(EigVec[:,1:pos_enc_dim+1]).float()\n",
    "\n",
    "    return g\n",
    "\n",
    "def dataset_with_lap(dataset, N=500, nhead=nhead):\n",
    "    for i in range(len(dataset)):\n",
    "        if i < N:\n",
    "            g = dataset[i][0]\n",
    "\n",
    "            result = dataset[i][1]\n",
    "            g = laplacian_positional_encoding(g, pos_emb_dim)\n",
    "\n",
    "            number_of_nodes = g.ndata['feat'].shape[0]\n",
    "\n",
    "            # pad and add CLS\n",
    "\n",
    "            feat = torch.full(size=[40], fill_value=pad_val, dtype=g.ndata['feat'].dtype)\n",
    "            feat[0] = cls_val\n",
    "            feat[0:number_of_nodes] = g.ndata['feat']\n",
    "\n",
    "            lap_PE = torch.zeros(size=[40, pos_emb_dim], dtype=g.ndata['lap_pos_enc'].dtype)\n",
    "            lap_PE[0:number_of_nodes, :] = g.ndata['lap_pos_enc']\n",
    "\n",
    "            mask = torch.full(size=[40], fill_value=-np.inf)\n",
    "            mask[0:number_of_nodes] = 0\n",
    "            \n",
    "            adj = g.adjacency_matrix().to_dense() != 0\n",
    "            adj_large = torch.zeros(size=[40,40], dtype=np.bool)\n",
    "            adj_large[0:number_of_nodes, 0:number_of_nodes] = adj\n",
    "            connectivity_mask = torch.stack([adj_large]*nhead)\n",
    "\n",
    "            yield (feat, lap_PE, result, mask, connectivity_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba70512f8cc4230866c920bcab3c993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omrib\\miniconda3\\envs\\graph_transformer\\lib\\site-packages\\dgl\\base.py:45: DGLWarning: DGLGraph.adjacency_matrix_scipy is deprecated. Please replace it with:\n",
      "\n",
      "\tDGLGraph.adjacency_matrix(transpose, scipy_fmt=\"csr\").\n",
      "\n",
      "  return warnings.warn(message, category=category, stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a687470d3f884bcda1a778b82ca605bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader([g for g in tqdm(dataset_with_lap(dataset_train, 500))],\n",
    "                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_loader = DataLoader([g for g in tqdm(dataset_with_lap(dataset_val, 500))],\n",
    "                    batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, d, nlayers=12):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.node_feature_embedding = nn.Embedding(num_embeddings=num_atom_type+2, embedding_dim=d) # +1 for the cls, which is the last one.\n",
    "        self.pos_embedding_linear = nn.Linear(in_features=pos_emb_dim, out_features=d)\n",
    "\n",
    "        tlayer = nn.TransformerEncoderLayer(d_model=d, nhead=nhead, dim_feedforward=d*4, dropout=0.1, activation='relu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(tlayer, nlayers)\n",
    "        self.transformer_encoder_layer = tlayer\n",
    "\n",
    "        self.out = nn.Linear(in_features=d, out_features=1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, node_feat, lap_PE, padd_mask, conn_mask):\n",
    "        \"\"\"\n",
    "        node_feat is (batch x nodes x 28)\n",
    "        lap_PE is (batch x node x 8)\n",
    "        \"\"\"\n",
    "        batch_size = node_feat.shape[0]\n",
    "        node_feat = self.node_feature_embedding(node_feat)\n",
    "\n",
    "        # move the laplacien PE to the 'd' dimension\n",
    "        lap_PE = self.pos_embedding_linear(lap_PE)\n",
    "\n",
    "#         activations = lap_PE + node_feat\n",
    "        activations = node_feat\n",
    "\n",
    "        vprint('---------------------')\n",
    "        vprint(f'activation size:{activations.shape}')\n",
    "        vprint(f'mask size:{padd_mask.shape}')\n",
    "        activations = activations.permute(1,0,2)\n",
    "        vprint(f'permuted activation size:{activations.shape}')\n",
    "        activations = self.transformer_encoder(activations, src_key_padding_mask=padd_mask, mask=conn_mask)\n",
    "        vprint(f'transfrormed activation size:{activations.shape}')\n",
    "        activations = activations.permute(1,0,2)\n",
    "        vprint(f'permuted back activation size:{activations.shape}')\n",
    "        activations = torch.sum(activations, dim=1)\n",
    "        vprint(f'summed activation size:{activations.shape}')\n",
    "        result = self.out(activations)\n",
    "        vprint(f'result size:{result.shape}')\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(d=32, nlayers=1).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000070)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                 factor=0.5,\n",
    "                                                 patience=10,\n",
    "                                                 verbose=True)\n",
    "\n",
    "def train_epoch(model):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in train_loader:\n",
    "        node_feat = batch[0].to(device)\n",
    "#         print(node_feat)\n",
    "        pos_emb = batch[1].to(device)\n",
    "        targets = batch[2].to(device)\n",
    "        mask = batch[3].to(device)\n",
    "        connecticity_mask = batch[4].to(device).view(-1, 40, 40)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model.forward(node_feat, pos_emb, mask, connecticity_mask)\n",
    "        loss = nn.L1Loss()(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_epoch(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in val_loader:\n",
    "        node_feat = batch[0].to(device)\n",
    "        pos_emb = batch[1].to(device)\n",
    "        targets = batch[2].to(device)\n",
    "        mask = batch[3].to(device)\n",
    "        connecticity_mask = batch[4].to(device).view(-1, 40, 40)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model.forward(node_feat, pos_emb, mask, connecticity_mask)\n",
    "            loss = nn.L1Loss()(preds, targets)\n",
    "\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 26.88532257080078, validation loss 26.27253532409668\n",
      "epoch 1, train loss 25.87335777282715, validation loss 25.223161697387695\n",
      "epoch 2, train loss 24.815196990966797, validation loss 24.170955657958984\n",
      "epoch 3, train loss 23.76415252685547, validation loss 23.115629196166992\n",
      "epoch 4, train loss 22.72788429260254, validation loss 22.057022094726562\n",
      "epoch 5, train loss 21.67156410217285, validation loss 20.99496078491211\n",
      "epoch 6, train loss 20.643638610839844, validation loss 19.929031372070312\n",
      "epoch 7, train loss 19.551233291625977, validation loss 18.85898780822754\n",
      "epoch 8, train loss 18.53038787841797, validation loss 17.78445053100586\n",
      "epoch 9, train loss 17.450328826904297, validation loss 16.70499038696289\n",
      "epoch 10, train loss 16.351680755615234, validation loss 15.62044906616211\n",
      "epoch 11, train loss 15.304275512695312, validation loss 14.530267715454102\n",
      "epoch 12, train loss 14.179101943969727, validation loss 13.434110641479492\n",
      "epoch 13, train loss 13.115377426147461, validation loss 12.335062026977539\n",
      "epoch 14, train loss 11.998175621032715, validation loss 11.230457305908203\n",
      "epoch 15, train loss 10.898911476135254, validation loss 10.125099182128906\n",
      "epoch 16, train loss 9.792988777160645, validation loss 9.02333927154541\n",
      "epoch 17, train loss 8.68229866027832, validation loss 7.930484771728516\n",
      "epoch 18, train loss 7.555818557739258, validation loss 6.855868339538574\n",
      "epoch 19, train loss 6.528573989868164, validation loss 5.841462135314941\n",
      "epoch 20, train loss 5.437685012817383, validation loss 4.8995184898376465\n",
      "epoch 21, train loss 4.623234748840332, validation loss 4.094277381896973\n",
      "epoch 22, train loss 3.899658679962158, validation loss 3.5090930461883545\n",
      "epoch 23, train loss 3.3768563270568848, validation loss 3.163220167160034\n",
      "epoch 24, train loss 3.1418797969818115, validation loss 3.0461950302124023\n",
      "epoch 25, train loss 3.009464740753174, validation loss 3.087798833847046\n",
      "epoch 26, train loss 3.023573398590088, validation loss 3.1755151748657227\n",
      "epoch 27, train loss 3.0641121864318848, validation loss 3.2650227546691895\n",
      "epoch 28, train loss 3.2080464363098145, validation loss 3.3272106647491455\n",
      "epoch 29, train loss 3.255216121673584, validation loss 3.3402936458587646\n",
      "epoch 30, train loss 3.222290277481079, validation loss 3.296872615814209\n",
      "epoch 31, train loss 3.1979660987854004, validation loss 3.2123990058898926\n",
      "epoch 32, train loss 3.0785632133483887, validation loss 3.100367546081543\n",
      "epoch 33, train loss 2.9857397079467773, validation loss 2.975963592529297\n",
      "epoch 34, train loss 2.868292808532715, validation loss 2.855038642883301\n",
      "epoch 35, train loss 2.7786130905151367, validation loss 2.7511889934539795\n",
      "epoch 36, train loss 2.638169288635254, validation loss 2.67018461227417\n",
      "epoch 37, train loss 2.5949509143829346, validation loss 2.6082358360290527\n",
      "epoch 38, train loss 2.5963525772094727, validation loss 2.564858913421631\n",
      "epoch 39, train loss 2.5244083404541016, validation loss 2.5325927734375\n",
      "epoch 40, train loss 2.5333924293518066, validation loss 2.5051991939544678\n",
      "epoch 41, train loss 2.510469436645508, validation loss 2.4741594791412354\n",
      "epoch 42, train loss 2.50313401222229, validation loss 2.4388418197631836\n",
      "epoch 43, train loss 2.405167818069458, validation loss 2.4004435539245605\n",
      "epoch 44, train loss 2.4064784049987793, validation loss 2.3596909046173096\n",
      "epoch 45, train loss 2.3770813941955566, validation loss 2.3192191123962402\n",
      "epoch 46, train loss 2.3279247283935547, validation loss 2.2816736698150635\n",
      "epoch 47, train loss 2.28485107421875, validation loss 2.246342658996582\n",
      "epoch 48, train loss 2.2150282859802246, validation loss 2.21335506439209\n",
      "epoch 49, train loss 2.2132577896118164, validation loss 2.1823196411132812\n",
      "epoch 50, train loss 2.185256004333496, validation loss 2.1529860496520996\n",
      "epoch 51, train loss 2.1696524620056152, validation loss 2.1241250038146973\n",
      "epoch 52, train loss 2.1383743286132812, validation loss 2.0944643020629883\n",
      "epoch 53, train loss 2.0798704624176025, validation loss 2.064974784851074\n",
      "epoch 54, train loss 2.0871520042419434, validation loss 2.0359060764312744\n",
      "epoch 55, train loss 2.0315685272216797, validation loss 2.0086069107055664\n",
      "epoch 56, train loss 2.0425913333892822, validation loss 1.9827334880828857\n",
      "epoch 57, train loss 2.0475621223449707, validation loss 1.9579538106918335\n",
      "epoch 58, train loss 2.0087199211120605, validation loss 1.9343512058258057\n",
      "epoch 59, train loss 1.964007019996643, validation loss 1.9121586084365845\n",
      "epoch 60, train loss 1.9276330471038818, validation loss 1.891080379486084\n",
      "epoch 61, train loss 1.8896864652633667, validation loss 1.870333194732666\n",
      "epoch 62, train loss 1.8839912414550781, validation loss 1.8504269123077393\n",
      "epoch 63, train loss 1.8487331867218018, validation loss 1.831536054611206\n",
      "epoch 64, train loss 1.853097915649414, validation loss 1.8132379055023193\n",
      "epoch 65, train loss 1.8268219232559204, validation loss 1.7965394258499146\n",
      "epoch 66, train loss 1.8331553936004639, validation loss 1.7809405326843262\n",
      "epoch 67, train loss 1.804586410522461, validation loss 1.765787959098816\n",
      "epoch 68, train loss 1.8157649040222168, validation loss 1.7527580261230469\n",
      "epoch 69, train loss 1.7855644226074219, validation loss 1.7415777444839478\n",
      "epoch 70, train loss 1.786610722541809, validation loss 1.7308467626571655\n",
      "epoch 71, train loss 1.7920703887939453, validation loss 1.7211946249008179\n",
      "epoch 72, train loss 1.7330024242401123, validation loss 1.7127466201782227\n",
      "epoch 73, train loss 1.7703156471252441, validation loss 1.7040419578552246\n",
      "epoch 74, train loss 1.7035748958587646, validation loss 1.6962430477142334\n",
      "epoch 75, train loss 1.7303872108459473, validation loss 1.6884982585906982\n",
      "epoch 76, train loss 1.684199571609497, validation loss 1.6809844970703125\n",
      "epoch 77, train loss 1.7176830768585205, validation loss 1.6741811037063599\n",
      "epoch 78, train loss 1.6724934577941895, validation loss 1.6679984331130981\n",
      "epoch 79, train loss 1.6739187240600586, validation loss 1.6621721982955933\n",
      "epoch 80, train loss 1.6712839603424072, validation loss 1.6564974784851074\n",
      "epoch 81, train loss 1.668850302696228, validation loss 1.6513330936431885\n",
      "epoch 82, train loss 1.6566556692123413, validation loss 1.647301197052002\n",
      "epoch 83, train loss 1.6631416082382202, validation loss 1.6438617706298828\n",
      "epoch 84, train loss 1.658651351928711, validation loss 1.6400165557861328\n",
      "epoch 85, train loss 1.6308221817016602, validation loss 1.635786533355713\n",
      "epoch 86, train loss 1.6274583339691162, validation loss 1.6312973499298096\n",
      "epoch 87, train loss 1.6245908737182617, validation loss 1.6263604164123535\n",
      "epoch 88, train loss 1.6110408306121826, validation loss 1.6219403743743896\n",
      "epoch 89, train loss 1.6608061790466309, validation loss 1.6179430484771729\n",
      "epoch 90, train loss 1.625741958618164, validation loss 1.6144688129425049\n",
      "epoch 91, train loss 1.6208280324935913, validation loss 1.610445261001587\n",
      "epoch 92, train loss 1.6246089935302734, validation loss 1.605325698852539\n",
      "epoch 93, train loss 1.603726863861084, validation loss 1.601241111755371\n",
      "epoch 94, train loss 1.6272261142730713, validation loss 1.5974280834197998\n",
      "epoch 95, train loss 1.5708024501800537, validation loss 1.5938615798950195\n",
      "epoch 96, train loss 1.5687570571899414, validation loss 1.5906500816345215\n",
      "epoch 97, train loss 1.5779540538787842, validation loss 1.5872256755828857\n",
      "epoch 98, train loss 1.5534415245056152, validation loss 1.5839838981628418\n",
      "epoch 99, train loss 1.581553339958191, validation loss 1.580338478088379\n",
      "epoch 100, train loss 1.6157035827636719, validation loss 1.5754421949386597\n",
      "epoch 101, train loss 1.5868149995803833, validation loss 1.5711965560913086\n",
      "epoch 102, train loss 1.6216092109680176, validation loss 1.5663195848464966\n",
      "epoch 103, train loss 1.600980520248413, validation loss 1.5618033409118652\n",
      "epoch 104, train loss 1.5484237670898438, validation loss 1.5569367408752441\n",
      "epoch 105, train loss 1.5599303245544434, validation loss 1.5515670776367188\n",
      "epoch 106, train loss 1.559739112854004, validation loss 1.5478434562683105\n",
      "epoch 107, train loss 1.5352778434753418, validation loss 1.544555902481079\n",
      "epoch 108, train loss 1.534061312675476, validation loss 1.5398569107055664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 109, train loss 1.54024338722229, validation loss 1.5347416400909424\n",
      "epoch 110, train loss 1.5987354516983032, validation loss 1.5307623147964478\n",
      "epoch 111, train loss 1.5288629531860352, validation loss 1.5276471376419067\n",
      "epoch 112, train loss 1.5452933311462402, validation loss 1.5256762504577637\n",
      "epoch 113, train loss 1.505205512046814, validation loss 1.5209732055664062\n",
      "epoch 114, train loss 1.5039116144180298, validation loss 1.5153065919876099\n",
      "epoch 115, train loss 1.5313224792480469, validation loss 1.5106122493743896\n",
      "epoch 116, train loss 1.5493954420089722, validation loss 1.5051977634429932\n",
      "epoch 117, train loss 1.533901572227478, validation loss 1.5006155967712402\n",
      "epoch 118, train loss 1.54325532913208, validation loss 1.496657371520996\n",
      "epoch 119, train loss 1.5305795669555664, validation loss 1.4936954975128174\n",
      "epoch 120, train loss 1.5037662982940674, validation loss 1.4900810718536377\n",
      "epoch 121, train loss 1.5173838138580322, validation loss 1.4849679470062256\n",
      "epoch 122, train loss 1.520071268081665, validation loss 1.4794514179229736\n",
      "epoch 123, train loss 1.4775394201278687, validation loss 1.4757416248321533\n",
      "epoch 124, train loss 1.480216383934021, validation loss 1.4731028079986572\n",
      "epoch 125, train loss 1.4959757328033447, validation loss 1.470118522644043\n",
      "epoch 126, train loss 1.4891670942306519, validation loss 1.466771125793457\n",
      "epoch 127, train loss 1.4433250427246094, validation loss 1.4616072177886963\n",
      "epoch 128, train loss 1.4648072719573975, validation loss 1.4559078216552734\n",
      "epoch 129, train loss 1.5116095542907715, validation loss 1.451983094215393\n",
      "epoch 130, train loss 1.4514859914779663, validation loss 1.4489959478378296\n",
      "epoch 131, train loss 1.4414072036743164, validation loss 1.4465773105621338\n",
      "epoch 132, train loss 1.473924160003662, validation loss 1.4434974193572998\n",
      "epoch 133, train loss 1.452265977859497, validation loss 1.4383165836334229\n",
      "epoch 134, train loss 1.490119218826294, validation loss 1.433271884918213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b6256b46bb67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-c233b0ff9f47>\u001b[0m in \u001b[0;36meval_epoch\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnecticity_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-51c8294c390e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, node_feat, lap_PE, padd_mask, conn_mask)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mvprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'permuted activation size:{activations.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadd_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconn_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mvprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'transfrormed activation size:{activations.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\graph_transformer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\graph_transformer\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\graph_transformer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\graph_transformer\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m         \u001b[0msrc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\graph_transformer\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    train_loss = train_epoch(model)\n",
    "    val_loss = eval_epoch(model)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "#     if i % 20 == 19:\n",
    "    print(f'epoch {i}, train loss {train_loss}, validation loss {val_loss}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
